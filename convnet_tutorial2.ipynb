{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "convnet_tutorial2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "IvnNS-iCRmjs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Click the link to do the tutorial in Colab:\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/akhti/ammi-2019-ConvNets/blob/master/convnet_tutorial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "dCtrnR_JR3pc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Standard imports and utiliy functions:"
      ]
    },
    {
      "metadata": {
        "id": "hqnl0AKVXIA4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "import PIL\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import skimage.transform\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from IPython import display\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "def display_thumb(img):\n",
        "  display.display(transforms.Resize(128)(img))\n",
        "  \n",
        "  \n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dzfEE578uSNp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if device != 'cuda':\n",
        "  print('WARNING! No cuda found, the executions may be slow')\n",
        "  print('If you are using colab, switch to GPU runtime.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zgE0byUgKwM6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load MNIST and define train/test functions as before."
      ]
    },
    {
      "metadata": {
        "id": "-NMUce6PKu-o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load the training and test dataset.\n",
        "mnist_train = datasets.MNIST('/tmp/mnist', train=True, download=True, transform=transforms.ToTensor())\n",
        "mnist_test = datasets.MNIST('/tmp/mnist', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Size of the batches the data loader will produce.\n",
        "batch_size = 64\n",
        "\n",
        "# This creates the dataloaders.\n",
        "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TgAJ94UgK1VU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(model, criterion, data_loader, optimizer, num_epochs):\n",
        "    \"\"\"Simple training loop for a PyTorch model.\"\"\"  \n",
        "    # Make sure model is in training mode.\n",
        "    model.train()\n",
        "    # Move model to the device.\n",
        "    model.to(device)\n",
        "    \n",
        "    # Exponential moving average of the loss.\n",
        "    ema_loss = None\n",
        "    \n",
        "    # Loop over epochs.\n",
        "    for epoch in range(num_epochs):    \n",
        "      # Loop over data.\n",
        "      for batch_idx, (data, target) in enumerate(data_loader):\n",
        "          # Forward pass.\n",
        "          output = model(data.to(device))\n",
        "          loss = criterion(output.to(device), target.to(device))\n",
        "          \n",
        "          # Backward pass.\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          \n",
        "          if ema_loss is None:\n",
        "            ema_loss = loss.item()\n",
        "          else:\n",
        "            ema_loss += (loss.item() - ema_loss) * 0.01 \n",
        "          \n",
        "      # Print out progress the end of epoch.\n",
        "      print('Train Epoch: {} \\tLoss: {:.6f}'.format(\n",
        "            epoch, ema_loss),\n",
        "      )\n",
        "              \n",
        "              \n",
        "def test(model, data_loader):\n",
        "    \"\"\"Measures the accuracy of a model on a data set.\"\"\" \n",
        "    # Make sure the model is in evaluation mode.\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "\n",
        "    # We do not need to maintain intermediate activations while testing.\n",
        "    with torch.no_grad():   \n",
        "        # Loop over test data.\n",
        "        for data, target in data_loader:\n",
        "          \n",
        "            # Forward pass.\n",
        "            output = model(data.to(device))\n",
        "            \n",
        "            # Get the label corresponding to the highest predicted probability.\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            \n",
        "            # Count number of correct predictions.\n",
        "            correct += pred.cpu().eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    # Print test accuracy.\n",
        "    percent = 100. * correct / len(data_loader.dataset)\n",
        "    print(f'Accuracy: {correct}/{len(data_loader.dataset)} ({percent:.0f}%)')\n",
        "    return percent\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GjTq5-k_q8s_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Convolutions and images\n",
        "The inputs and outputs of convolutional layers are 3D tensor. The number of the channels in the output tensor is defined explicitly with `out_channels` parameter. At the same time, the spatial dimensions of the ouput image (width and height) depend on the spacial dimensions of the input image, kernel_size, padding, and striding. In order to build efficient convolutional networks, it's important to understand what are the sizes after afterh each convolutional layer.\n",
        "\n",
        "In this exersise you will derive the dependency between input and output sizes. For the sake of simplicity we assume that the input tensor is _square_, i.e., width = height = image_size.\n",
        "\n",
        "If your code is correct, you should see 'OK'."
      ]
    },
    {
      "metadata": {
        "id": "5t4hWoUYpp05",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_convnet_output_size(image_size, kernel_size, padding, stride):\n",
        "  ###########################################################################\n",
        "  # TODO: Compute the size of the image after a conv layer.                 #\n",
        "  # Put your code between START_GRADING and END_GRADING.                    #\n",
        "  ###########################################################################\n",
        "  # ~~START_GRADING~~\n",
        " \n",
        "  # ~~END_GRADING~~  \n",
        "  return output_size\n",
        "\n",
        "\n",
        "# Compare the size of the output of nn.Conv2d with compute_convnet_output_size.\n",
        "for image_size in range(5, 21, 1):\n",
        "  # Shape: batch x channels x height x width.\n",
        "  input_tensor = torch.zeros((1, 1, image_size, image_size))\n",
        "  for kernel_size in 2, 3, 5, 7:\n",
        "    for padding in 0, 5:\n",
        "      for stride in 1, 2, 3, 4:\n",
        "        if kernel_size >= image_size:\n",
        "          continue\n",
        "        output_tensor = nn.Conv2d(1, 1, kernel_size, stride, padding)(input_tensor)\n",
        "        output_size = output_tensor.size(2)\n",
        "        predicted_output_size = compute_convnet_output_size(\n",
        "            image_size, kernel_size, padding, stride)\n",
        "        assert output_size == predicted_output_size, (\n",
        "            f'ERROR: the real size is {output_size},'\n",
        "            f' but got {predicted_output_size}.'\n",
        "            f'\\nimage_size={image_size}'\n",
        "            f' kernel_size={kernel_size}'\n",
        "            f' padding={padding}'\n",
        "            f' stride={stride}'\n",
        "        )\n",
        "\n",
        "print('OK')\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "71LRkRxndajG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BatchNorm\n"
      ]
    },
    {
      "metadata": {
        "id": "oN8qTuGkZ-3X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Batch normalization is tenchique that allows to make training more stable fast [1].\n",
        "\n",
        "Below we define a convolutional network with 3 layers. Before each ReLU layer we insert a BatchNorm2d layer if `use_batch_norm` is `True`. This improves the convergence as guarantees as values have the same variance asn zero-means. As a result on average exactly half of the values will be nulled by ReLU.\n",
        "\n",
        "[1] Ioffe, Sergey, and Christian Szegedy. \"Batch normalization: Accelerating deep network training by reducing internal covariate shift.\" arXiv preprint arXiv:1502.03167 (2015).\n",
        "\n",
        "**Task**. Go ahead and add batch normalization layer."
      ]
    },
    {
      "metadata": {
        "id": "vg4CO_WDeLqh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ConvolutionalNetworkWithBN(nn.Module):\n",
        "  \"\"\"Convolutional network that optional batch normalization.\"\"\"\n",
        "  \n",
        "  def __init__(self, use_batch_norm=False):\n",
        "      super().__init__()\n",
        "      num_channels = 8\n",
        "      self.use_batch_norm = use_batch_norm\n",
        "      # We define all our layers in a single Sequential. If use_batch_norm is \n",
        "      # True, we'd like maybe_batch_norm_layer to produce a batchnorm layer.\n",
        "      self.conv = nn.Sequential(\n",
        "          # 1x28x28 -> 8x24x24.\n",
        "          nn.Conv2d(1, num_channels, kernel_size=5),\n",
        "          # 8x24x24 -> 8x12x12.\n",
        "          nn.MaxPool2d(2),\n",
        "          self.maybe_batch_norm_layer(num_channels),\n",
        "          nn.ReLU(inplace=True),\n",
        "          nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1),          \n",
        "          self.maybe_batch_norm_layer(num_channels),            \n",
        "          nn.ReLU(inplace=True),\n",
        "          # 8x12x12 -> 8x8x8.\n",
        "          nn.Conv2d(num_channels, num_channels, kernel_size=5),\n",
        "          # 8x8x8 -> 8x4x4.\n",
        "          nn.MaxPool2d(2),\n",
        "          self.maybe_batch_norm_layer(num_channels),\n",
        "          nn.ReLU(inplace=True),\n",
        "      )\n",
        "      self.linear = nn.Linear(num_channels * 4 ** 2, 10)  \n",
        "      \n",
        "  def maybe_batch_norm_layer(self, num_channels):\n",
        "      if self.use_batch_norm:\n",
        "          ###########################################################################\n",
        "          # TODO: Add batch norm layer                                              #\n",
        "          # Put your code between START_GRADING and END_GRADING.                    #\n",
        "          ###########################################################################\n",
        "          # ~~START_GRADING~~\n",
        "          return BUILD_BATCH_NORM2D_LAYER_HERE\n",
        "          # ~~END_GRADING~~\n",
        "      else:\n",
        "          # This layer will copy its input to the output.\n",
        "          return nn.Sequential()\n",
        "      \n",
        "  def forward(self, x):\n",
        "      x = self.conv(x)  \n",
        "      x = self.linear(x.view(x.size(0), -1))\n",
        "      return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qbtzzBhiJ4PW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To see how batch normalization improves stability, let's try to train the neural network with different learning rates and check the accuracies."
      ]
    },
    {
      "metadata": {
        "id": "YXuSliJF3r7u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for lr in 0.3, 0.1, 0.03, 0.01, 0.003:\n",
        "  conv_model = ConvolutionalNetworkWithBN()\n",
        "  optimizer = torch.optim.SGD(conv_model.parameters(), lr=lr)\n",
        "  train(conv_model, nn.CrossEntropyLoss(), train_loader, optimizer, num_epochs=1)\n",
        "  accuracy = test(conv_model, test_loader)\n",
        "  if accuracy > 96:\n",
        "    print(f'##### lr={lr} is GOOD')\n",
        "  elif accuracy > 90:\n",
        "    print(f'##### lr={lr} is DESCENT')\n",
        "  else:\n",
        "    print(f'##### lr={lr} is BAD')   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WrJBxM03KOZ-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see learning rate should be in a narrow region to get GOOD result.\n",
        "\n",
        "Now let's try to run the same batch normalization. Before running the code you should finish TODO in ConvolutionalNetworkWithBN code. If your code is correct, then accuracies will improve."
      ]
    },
    {
      "metadata": {
        "id": "6u2JJdnX5VKF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for lr in 0.3, 0.1, 0.03, 0.01, 0.003:\n",
        "  conv_model = ConvolutionalNetworkWithBN(use_batch_norm=True)\n",
        "  optimizer = torch.optim.SGD(conv_model.parameters(), lr=lr)\n",
        "  train(conv_model, nn.CrossEntropyLoss(), train_loader, optimizer, num_epochs=1)\n",
        "  accuracy = test(conv_model, test_loader)\n",
        "  if accuracy > 96:\n",
        "    print(f'##### lr={lr} is GOOD (with batch norm)')\n",
        "  elif accuracy > 90:\n",
        "    print(f'##### lr={lr} is DESCENT (with batch norm)')\n",
        "  else:\n",
        "    print(f'##### lr={lr} is BAD (with batch norm)')   \n",
        "  if lr >= 0.01:\n",
        "    assert accuracy > 90, 'Accuracy is too low. Check your BatchNorm code'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gdgr7eNFTTAn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Question [optional]:** Replace the batch normalization layer in your network by your own implementation. Confirm your batch-normalization implementation is correct."
      ]
    },
    {
      "metadata": {
        "id": "MAuhUMtjTj3f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MyBatchNorm2d(nn.Module):\n",
        "    \"\"\"Simple implementation of batch normalization.\"\"\"\n",
        "\n",
        "    def __init__(self, num_channels, momentum=0.1, epsilon=1e-5):\n",
        "        super(MyBatchNorm2d, self).__init__()\n",
        "\n",
        "        # Initialize bias and gain parameters.\n",
        "        self.gamma = nn.Parameter(torch.ones(1, num_channels, 1, 1))\n",
        "        self.beta = nn.Parameter(torch.zeros(1, num_channels, 1, 1))\n",
        "\n",
        "        # Initialize moving averages.\n",
        "        self.epsilon = epsilon\n",
        "        self.momentum = momentum\n",
        "        self.register_buffer('running_mean', torch.zeros((1, num_channels, 1, 1)))\n",
        "        self.register_buffer('running_var', torch.ones((1, num_channels, 1, 1)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Check that input is of correct size.\n",
        "        assert x.dim() == 4, 'input should be NCHW'\n",
        "        assert x.size(1) == self.gamma.numel()\n",
        "        ########################################################################\n",
        "        # TODO: Add batch norm layer implementation.                           #\n",
        "        # You code should:                                                     #\n",
        "        #   * Compute per-channel mean and variance.                           #\n",
        "        #   * Update running_mean and running_var.                             #\n",
        "        #   * Compute normalized features.                                     #\n",
        "        # Put your code between START_GRADING and END_GRADING.                 #\n",
        "        ########################################################################\n",
        "        # ~~START_GRADING~~\n",
        "\n",
        "        # ~~END_GRADING~~\n",
        "\n",
        "\n",
        "# Use this code to test if your implementation is correct.\n",
        "batch_size, num_channels, im_size = 32, 8, 6\n",
        "batchnorm1 = nn.BatchNorm2d(num_channels)\n",
        "batchnorm2 = MyBatchNorm2d(num_channels)\n",
        "for key, param in batchnorm1.named_parameters():\n",
        "    if key == 'weight':\n",
        "        param.data.fill_(1.0)  # undo random initialization in nn.BatchNorm2d\n",
        "for mode in [True, False]:     # test in training and evaluation mode\n",
        "    batchnorm1.train(mode=mode)\n",
        "    batchnorm2.train(mode=mode)\n",
        "    for _ in range(5):\n",
        "        x = torch.randn(batch_size, num_channels, im_size, im_size) + 10.0\n",
        "        out1 = batchnorm1(x)\n",
        "        out2 = batchnorm2(x)\n",
        "        assert (batchnorm1.running_mean - batchnorm2.running_mean.squeeze()).abs().max() < 1e-5, \\\n",
        "            'running mean is incorrect (%s mode)' % ('train' if mode else 'eval')\n",
        "        assert (batchnorm1.running_var - batchnorm2.running_var.squeeze()).abs().max() < 1e-5, \\\n",
        "            'running variance is incorrect (%s mode)' % ('train' if mode else 'eval')\n",
        "        assert (out1 - out2).abs().max() < 5e-3, \\\n",
        "            'normalized output is incorrect (%s mode)' % ('train' if mode else 'eval')\n",
        "print('All OK!')\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ALURWmhbK7yM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ResNets\n",
        "\n",
        "We are going train a deep neural network using residual connections. [2]\n",
        "\n",
        "The network below has a list of `conv_blocks`. Each convolutional block is convolution layer followed by a ReLU with optional pooling.\n",
        "\n",
        "[2] He, Kaiming, et al. \"Identity mappings in deep residual networks.\" European conference on computer vision. Springer, Cham, 2016.\n",
        "\n",
        "**Task**. Implement resudual connections within `apply_conv_block`. Residual connection sum the output of a convolutional block with its input. Note, that if the output and input have different sizes, you should skip a residual connection."
      ]
    },
    {
      "metadata": {
        "id": "tp6FGIdgmDrG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DeepConvolutionalNetwork(nn.Module):\n",
        "  \"\"\"Convolutional network that can go arbitrary deep.\"\"\"\n",
        "  \n",
        "  def __init__(self, num_repeated_layers=1, use_residuals=False):\n",
        "    super().__init__()\n",
        "    num_channels = 8\n",
        "    self.use_residuals = use_residuals\n",
        "\n",
        "    conv_blocks = []\n",
        "    # 1x28x28 -> 8x12x12.\n",
        "    conv_blocks.append(nn.Sequential(\n",
        "      nn.Conv2d(1, num_channels, kernel_size=5),\n",
        "      nn.MaxPool2d(2),\n",
        "      nn.ReLU(inplace=True),\n",
        "    ))\n",
        "    for _ in range(num_repeated_layers):\n",
        "      conv_blocks.append(nn.Sequential(\n",
        "        nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1),          \n",
        "        nn.ReLU(inplace=True),\n",
        "      ))\n",
        "    # 8x12x12 -> 8x4x4.\n",
        "    conv_blocks.append(nn.Sequential(\n",
        "      nn.Conv2d(num_channels, num_channels, kernel_size=5),\n",
        "      nn.MaxPool2d(2),\n",
        "      nn.ReLU(inplace=True),\n",
        "    ))\n",
        "    # It's important to wrap layers into ModuleList so the PyTorch knows that\n",
        "    # the self.conv_blocks has parameters to optimize.\n",
        "    self.conv_blocks = nn.ModuleList(conv_blocks)\n",
        "    self.linear = nn.Linear(num_channels * 4 ** 2, 10)  \n",
        "   \n",
        "  def apply_conv_block(self, conv_block, x):\n",
        "    output = conv_block(x)\n",
        "    if not self.use_residuals:\n",
        "      return output\n",
        "    else:\n",
        "      ##########################################################################\n",
        "      # TODO: Add residual connection to output.                               #\n",
        "      # Note, if the input size is not equal to the output size, the residual  #\n",
        "      # connextion is not needed.                                              #\n",
        "      # Put your code between START_GRADING and END_GRADING.                   #\n",
        "      ##########################################################################\n",
        "      # ~~START_GRADING~~      \n",
        "\n",
        "      # ~~END_GRADING~~      \n",
        "      return output\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    for block in self.conv_blocks:\n",
        "      x = self.apply_conv_block(block, x) \n",
        "    x = self.linear(x.view(x.size(0), -1))\n",
        "    return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lRO8MhydMSOS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's look at the modules in our deep network."
      ]
    },
    {
      "metadata": {
        "id": "qInniW1-MXCW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Visualize a network with 3 inner layer.\n",
        "DeepConvolutionalNetwork(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7zLAIfS2Q4fM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we are going to train the model with increasing number of layers with and without residual connections. What do you observe?"
      ]
    },
    {
      "metadata": {
        "id": "6l6YAVuJRGbQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for num_layer in 1, 5, 10:\n",
        "  for use_residual in [False, True]:\n",
        "    print(f'Training a neural network with {num_layer} layers and use_residual={use_residual}')\n",
        "    conv_model = DeepConvolutionalNetwork(num_layer, use_residuals=use_residual)\n",
        "    optimizer = torch.optim.SGD(conv_model.parameters(), lr=1e-1)\n",
        "    train(conv_model, nn.CrossEntropyLoss(), train_loader, optimizer, num_epochs=1)\n",
        "    accuracy = test(conv_model, test_loader)\n",
        "    if use_residual:\n",
        "      assert accuracy > 90, 'Accuracy is very low. Check the implementaion of residuals'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CPDc4SCxARBI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Imagenet"
      ]
    },
    {
      "metadata": {
        "id": "W0vWX6U_4ge8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Imagenet is the most famous dataset for image classification that is still in use. Real ImageNet dataset is very big (~150Gb). So we will use a smaller version that contains only two classes: bees and ants.\n",
        "\n",
        "First, download the required files and consturct the dataset."
      ]
    },
    {
      "metadata": {
        "id": "BxBuHcp70tjX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! rm -rf /tmp/imagenet/unpacked\n",
        "! mkdir -p /tmp/imagenet/unpacked\n",
        "! [[ -f /tmp/imagenet/hymenoptera_data.zip ]] || wget https://download.pytorch.org/tutorial/hymenoptera_data.zip -O /tmp/imagenet/hymenoptera_data.zip\n",
        "! cd /tmp/imagenet/unpacked && unzip ../hymenoptera_data.zip > /dev/null\n",
        "imagenet_dataset = datasets.ImageFolder('/tmp/imagenet/unpacked/hymenoptera_data/val')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mCNAGpNF41mW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Explore the images within the dataset. Unlike MNIST or CIFAR the dataset has relatively high-resolution images that can vary in size."
      ]
    },
    {
      "metadata": {
        "id": "Fx4HoaMP30Ya",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('ImageNet classes:', *imagenet_dataset.classes)\n",
        "# Show a random image and the corresponding target.\n",
        "for i in range(3):\n",
        "  img, target = random.choice(imagenet_dataset)\n",
        "  print('Label of image: %d (%s). Original size: %s' % (target, imagenet_dataset.classes[target], img.size))\n",
        "  # Reduce image size by half to fit into the page :)\n",
        "  display.display(img.resize((img.size[0] // 2, img.size[1] // 2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x2n_zYU96dmU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A coomon tecnnique to handle this is to take some segment of the image and resize it to have an image of fixed squre size.\n",
        "\n",
        "For training the segment selection is usually randomized.\n",
        "For evalution a \"center\" crop is used."
      ]
    },
    {
      "metadata": {
        "id": "VyAClHzg3PKi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "img, _ = random.choice(imagenet_dataset)\n",
        "print('Actual image:')\n",
        "display.display(img)\n",
        "print('Input to the net for training')\n",
        "display.display(transforms.RandomResizedCrop(224)(img))\n",
        "print('Input to the net for eval')\n",
        "display.display(transforms.CenterCrop(224)(transforms.Resize(256)(img)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_pY7eXqw7BmK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If a pre-trained model is used, it's important to match the transformation."
      ]
    },
    {
      "metadata": {
        "id": "rUcHTuWN3Y1S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This is the default transform used in ImageNet models.\n",
        "inference_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# For visualization purposes we'll create a separate transform that operates in image space.\n",
        "inference_transform_show = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "])\n",
        "\n",
        "# Reload out dataset with this transform.\n",
        "trasformed_imagenet_dataset = datasets.ImageFolder(\n",
        "    '/tmp/imagenet/unpacked/hymenoptera_data/val',\n",
        "    transform=inference_transform,\n",
        ")\n",
        "\n",
        "trasformed_imagenet_loader = torch.utils.data.DataLoader(trasformed_imagenet_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Load a pretrained model.\n",
        "imagenet_resnet = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "# The loaded model is trained to classify image across 1000 classes. We have only two classes so have to take the two correpsonding lines from the softmax.\n",
        "ANT_CLASS_ID = 310\n",
        "BEE_CLASS_ID = 309\n",
        "imagenet_resnet.fc.weight.data = imagenet_resnet.fc.weight.data[[ANT_CLASS_ID, BEE_CLASS_ID]]\n",
        "imagenet_resnet.fc.bias.data = imagenet_resnet.fc.bias.data[[ANT_CLASS_ID, BEE_CLASS_ID]]\n",
        "\n",
        "# Check the model accuracy.\n",
        "test(imagenet_resnet.cuda(), trasformed_imagenet_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vusiegGD_OHa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The model quality is pretty high, but there are a few mistakes. Explore misclassified images below. Why do you think the network made a mistake?\n"
      ]
    },
    {
      "metadata": {
        "id": "rJtp6Jn582ZD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(len(trasformed_imagenet_dataset)):\n",
        "  img, _ = imagenet_dataset[i]\n",
        "  tensor, target = trasformed_imagenet_dataset[i]\n",
        "    \n",
        "  logits = imagenet_resnet(tensor.unsqueeze(0).cuda()).squeeze(0).cpu()\n",
        "  _, prediction = logits.max(-1)\n",
        "  if prediction != target:\n",
        "    print('Img id=%d. Excpected class %s, but predicted class %s.' % (\n",
        "        i,\n",
        "        imagenet_dataset.classes[target],\n",
        "        imagenet_dataset.classes[prediction]))\n",
        "    display_thumb(img)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4kPeyocHV0vD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you saw the random crop not always catches the object. Lets understand how does it affect the classification.\n",
        "\n",
        "**Task** . For a random image from `imagenet_dataset` sample a few random crops and find the most _bee-like_ and the most _ant-like_ ones. How do they look like? (Hint: take a look at the previous section)"
      ]
    },
    {
      "metadata": {
        "id": "U9paFPHUKdZA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "random_id = 40\n",
        "img, _ = imagenet_dataset[random_id]\n",
        "\n",
        "random_crops = [transforms.RandomResizedCrop(224)(img)\n",
        "                for _ in range(10)]\n",
        "def beeness(image):\n",
        "  # Conver image to tensor and apply default for the dataset mean-variance normalization.\n",
        "  image = transforms.ToTensor()(image)\n",
        "  image = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(image)\n",
        "  ###########################################################################\n",
        "  # TODO: Sort the random_crops by probability of bee-ness.                 #\n",
        "  # You need to compute logits and convert them to probabilities.           #\n",
        "  # Put your code between START_GRADING and END_GRADING.                    #\n",
        "  ###########################################################################\n",
        "  # ~~START_GRADING~~      \n",
        "  # ~~END_GRADING~~ \n",
        "  return probs[0].item()\n",
        "  \n",
        "random_crops = sorted(random_crops, key=beeness)\n",
        "\n",
        "print('Most ant-like'); display.display(random_crops[0])\n",
        "print('Most bee-like'); display.display(random_crops[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}